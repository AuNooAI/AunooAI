# Use Python 3.11 slim image as base
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Build arguments for app info
ARG APP_VERSION=unknown
ARG APP_GIT_BRANCH=unknown
ARG APP_BUILD_DATE=unknown

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    APP_VERSION=${APP_VERSION} \
    APP_GIT_BRANCH=${APP_GIT_BRANCH} \
    APP_BUILD_DATE=${APP_BUILD_DATE}

# Install system dependencies
RUN apt-get update \
    && apt-get install -y --no-install-recommends \
        build-essential \
        curl \
        gnupg \
    && rm -rf /var/lib/apt/lists/*

# Install Google Cloud SDK
RUN echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main" | \
    tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && \
    curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | \
    apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && \
    apt-get update && apt-get install -y google-cloud-sdk && \
    rm -rf /var/lib/apt/lists/*

# Create necessary directories
RUN mkdir -p app/data \
    app/config \
    templates \
    static \
    reports \
    scripts \
    tests \
    data

# Copy requirements first
COPY requirements.txt .

# Install Python dependencies including GCP libraries
RUN pip install --no-cache-dir -r requirements.txt && \
    pip install --no-cache-dir google-cloud-storage uvicorn

# Copy application code and other necessary files
COPY app/ app/
COPY templates/ templates/
COPY static/ static/
COPY scripts/ scripts/
COPY tests/ tests/
COPY data/ data/
COPY .env /app/.env.example
COPY cloud_run_start.py /app/

# Ensure cloud_run_start.py is executable
RUN chmod +x /app/cloud_run_start.py

# Create config directory if it doesn't exist in source
RUN mkdir -p /app/config
RUN echo '# Configuration settings for AunooAI' > /app/config/__init__.py
RUN echo 'DATABASE_DIR = "app/data"' > /app/config/settings.py
RUN echo 'config = {"environment": "production"}' >> /app/config/settings.py

# Ensure the scripts directory exists - even if it wasn't copied
RUN mkdir -p /app/scripts
RUN touch /app/scripts/__init__.py
RUN echo '# Stub implementation of DatabaseMerger for Cloud Run' > /app/scripts/db_merge.py
RUN echo 'class DatabaseMerger:' >> /app/scripts/db_merge.py
RUN echo '    def __init__(self, *args, **kwargs):' >> /app/scripts/db_merge.py
RUN echo '        print("Using stub DatabaseMerger from scripts/db_merge.py")' >> /app/scripts/db_merge.py
RUN echo '' >> /app/scripts/db_merge.py
RUN echo '    def merge_databases(self, source_db_path):' >> /app/scripts/db_merge.py
RUN echo '        print(f"STUB: Would merge database from {source_db_path}")' >> /app/scripts/db_merge.py
RUN echo '        return True' >> /app/scripts/db_merge.py

# Ensure data directories exist and have proper permissions
RUN mkdir -p /app/app/data && \
    mkdir -p /app/reports
    
# Set permissions for directories
RUN chmod -R 777 /app/app/data /app/reports /app/data /app/scripts

# Create entrypoint script with GCP Cloud Storage sync
RUN echo '#!/bin/bash\n\
set -e\n\
\n\
echo "======== Starting AunooAI container ========"\n\
echo "Current directory: $(pwd)"\n\
echo "Environment variables:"\n\
echo "INSTANCE: ${INSTANCE}"\n\
echo "PORT: ${PORT}"\n\
echo "CONTAINER_PORT: ${CONTAINER_PORT}"\n\
echo "STORAGE_BUCKET: ${STORAGE_BUCKET}"\n\
\n\
# Create data directory for tenant\n\
mkdir -p /app/app/data/${INSTANCE}\n\
mkdir -p /app/reports/${INSTANCE}\n\
touch /app/app/data/${INSTANCE}/.env\n\
chmod 666 /app/app/data/${INSTANCE}/.env\n\
ln -sf /app/app/data/${INSTANCE}/.env /app/.env\n\
echo "Created data directory for tenant: ${INSTANCE}"\n\
echo "Created reports directory for tenant: ${INSTANCE}"\n\
\n\
# If a storage bucket is specified, sync data from it\n\
if [ -n "$STORAGE_BUCKET" ]; then\n\
  echo "Syncing data from gs://${STORAGE_BUCKET}/data/ to /app/app/data/${INSTANCE}/..."\n\
  gsutil -m rsync -r gs://${STORAGE_BUCKET}/data/ /app/app/data/${INSTANCE}/\n\
  \n\
  echo "Syncing reports from gs://${STORAGE_BUCKET}/reports/ to /app/reports/${INSTANCE}/..."\n\
  gsutil -m rsync -r gs://${STORAGE_BUCKET}/reports/ /app/reports/${INSTANCE}/\n\
  \n\
  # Set up periodic sync to bucket (every 5 minutes) and on exit\n\
  function sync_data_to_bucket() {\n\
    echo "$(date): Syncing data to gs://${STORAGE_BUCKET}/data/..."\n\
    gsutil -m rsync -r /app/app/data/${INSTANCE}/ gs://${STORAGE_BUCKET}/data/\n\
    \n\
    echo "$(date): Syncing reports to gs://${STORAGE_BUCKET}/reports/..."\n\
    gsutil -m rsync -r /app/reports/${INSTANCE}/ gs://${STORAGE_BUCKET}/reports/\n\
    \n\
    echo "$(date): Sync completed"\n\
  }\n\
  \n\
  # Set up exit trap\n\
  trap sync_data_to_bucket EXIT\n\
  \n\
  # Start periodic sync in background\n\
  (while true; do\n\
    sleep 300  # 5 minutes\n\
    sync_data_to_bucket\n\
  done) &\n\
  SYNC_PID=$!\n\
  echo "Started periodic sync process (PID: $SYNC_PID)"\n\
fi\n\
\n\
# PORT handling: Cloud Run sets PORT automatically and we should use that\n\
# For Kubernetes deployments, we use CONTAINER_PORT instead\n\
if [ -z "$PORT" ] && [ -n "$CONTAINER_PORT" ]; then\n\
  export PORT="$CONTAINER_PORT"\n\
  echo "Using CONTAINER_PORT value for PORT: ${PORT}"\n\
else\n\
  # Cloud Run will have already set PORT\n\
  echo "Using provided PORT: ${PORT}"\n\
fi\n\
\n\
# Make sure PORT is set to something if neither variable was provided\n\
export PORT="${PORT:-8080}"\n\
# Set environment variables for the application\n\
export ENVIRONMENT="production"\n\
# EXPLICITLY disable SSL for Cloud Run\n\
export DISABLE_SSL="true"\n\
export CERT_PATH="/dev/null"\n\
export KEY_PATH="/dev/null"\n\
echo "Final PORT value: ${PORT}"\n\
echo "DISABLE_SSL: ${DISABLE_SSL}"\n\
\n\
# Set initial admin password if ADMIN_PASSWORD is provided\n\
if [ -n "$ADMIN_PASSWORD" ]; then\n\
  echo "Setting initial admin password..."\n\
  cd /app && python -c "from app.utils.update_admin import update_admin_password; update_admin_password(\"/app/app/data/${INSTANCE}/fnaapp.db\", \"$ADMIN_PASSWORD\")"\n\
fi\n\
\n\
# Verify existence of key files\n\
if [ ! -d "/app/scripts" ]; then\n\
  echo "ERROR: scripts directory not found! Creating it..."\n\
  mkdir -p /app/scripts\n\
  touch /app/scripts/__init__.py\n\
  echo "class DatabaseMerger:\\n    def __init__(self, *args, **kwargs):\\n        print(\\"Stub DatabaseMerger created on-the-fly in entrypoint\\")\\n    def merge_databases(self, source_db_path):\\n        print(f\\"STUB: Would merge database from {source_db_path}\\")\\n        return True" > /app/scripts/db_merge.py\n\
fi\n\
\n\
echo "Content of /app/scripts:"\n\
ls -la /app/scripts/\n\
\n\
# List files to verify the application is properly installed\n\
echo "Application directory content:"\n\
ls -la /app/app/\n\
\n\
# Check if cloud_run_start.py exists\n\
if [ ! -f "/app/cloud_run_start.py" ]; then\n\
  echo "ERROR: /app/cloud_run_start.py not found!"\n\
  echo "Files in /app:"\n\
  find /app -type f | sort\n\
  exit 1\n\
fi\n\
\n\
echo "Starting application in Cloud Run mode on port ${PORT}"\n\
# Start using our Cloud Run-specific startup script\n\
cd /app && exec python /app/cloud_run_start.py' > /entrypoint.sh && chmod +x /entrypoint.sh

# Use the PORT environment variable for the application
# PORT is set automatically by Cloud Run, but we may use CONTAINER_PORT in Kubernetes
ENV PORT 8080
EXPOSE ${PORT}

# Command to run the application
CMD ["/entrypoint.sh"] 